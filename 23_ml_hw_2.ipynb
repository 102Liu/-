{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div align='center'><font size='6'> Fall 2023: Machine Learning</font></div>\n",
    "<div align='center'><font size='6'><b> Homework Set 2: Linear Models</b></font></div>\n",
    "<br>\n",
    "<div align='center'><font size='4'> Instructor: Lu Sheng</font></div>\n",
    "<div align='center'><font size='4'> School of Software, Beihang University</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Instructions\n",
    "\n",
    "* Upload a .ipynb file named as `id_yourname.ipynb` onto this **course** in [北京航空航天大学在线教学平台](https://spoc.buaa.edu.cn/), which should include\n",
    "\n",
    "    1. your answers to the [questions](#Questions) (can be written in markdown/code format or printed as images in this jupyter notebook),\n",
    "    \n",
    "    2. the codes, results, and/or discussions if required according [programming projects](#Programming-Project). \n",
    "    \n",
    "    For more details about python and Jupyter Notebook, as well as some useful packages (such as numpy and matplotlib), please check the following [link](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n",
    "* The deadline is 00:00, November 18, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Suppose that we have the option either to assign the pattern to one of $c$ classes, or to reject it as\n",
    "being unrecognizable. Let\n",
    "    $$\\lambda\\left(\\alpha_i \\mid \\omega_j\\right)= \\begin{cases}0 & i=j, \\quad i, j=1, \\ldots, c \\\\ \\lambda_r & i=c+1 \\\\ \\lambda_s & \\text { otherwise }\\end{cases}$$\n",
    "    where $\\lambda_r$ is the risk incurred for choosing the (c + 1)-th action (rejection).\n",
    "    \n",
    "    (a) Derive the decision rule in order to minimize the risk.\n",
    "\n",
    "    (b) What happens if $\\lambda_r$ = 0? What happens if $\\lambda_r$ > $\\lambda_s$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "(a) 风险的期望 $R$可以表示为：\n",
    "$$R = \\sum_{i=1}^{c} \\sum_{j=1}^{c} \\lambda(\\alpha_i|\\omega_j) P(\\omega_j|x) P(\\omega_j) + \\lambda_r P(\\alpha_{c+1})$$\n",
    "其中，$P(\\omega_j|x)$ 表示给定模式 $x$ 的条件下，模式属于类别 $\\omega_j$ 的概率，$P(\\omega_j)$ 表示类别 $\\omega_j$ 的先验概率，$P(\\alpha_{c+1})$ 表示拒绝动作的概率。\n",
    "为了最小化期望风险，需要选择使 $R$ 最小的决策规则。\n",
    "\n",
    "(b) 如果 $\\lambda_r = 0$，则拒绝动作不会产生任何风险。在这种情况下，决策规则会无脑拒绝分类，从而起不到分类的作用。\n",
    "\n",
    "如果 $\\lambda_r > \\lambda_s$，意味着选择拒绝动作的风险高于选择其他动作的风险。在这种情况下，决策规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a linear regression. Start by implementing the following functions:\n",
    "\n",
    "    - a function `theta = linear_regress(y, X)` where `X` and `y` are $n\\times d$ and $n \\times 1$ matrices respectively. This function trains a linear regression model on a training set of $n$ samples, each of which is a $d$-dimensional vector. The ground truth values for the examples are in `y` and are scalar values. The function should return theta as the final model parameter. Note that we are NOT explicitly including the offset parameter but instead rely on the feature vectors to provide a constant component, i.e., $\\mathbf{y} = \\boldsymbol\\theta^\\top \\mathbf{x}$.\n",
    "    - a function `y_hat = linear_pred(theta, X_test)` that predicts the `y_hat` from the test data `X_test`.\n",
    "\n",
    "    For this problem, we have provided you a custom-created dataset named `p2.mat`, which can be loaded by `scipy.io.loadmat()`.\n",
    "\n",
    "    (a) Load data from `p2.mat`, which should include loading the matrices `y_noisy`, `y_true`, `X_in`. The `y` vectors are $n \\times 1$ while `X_in` is a $n \\times 3$ matrix with each row corresponding to a point in $\\mathcal{R}^3$. The $y_{true}$ vectors correspond to the ideal $y$ values, generated directly from the ``true'' model (whatever it may be) without any noise. In contrast, the $y_{noisy}$ vectors are the actual, noisy observations, generated by adding Gaussian noise to the $y_{true}$ vectors. You should use $y_{noisy}$ for any estimation. $y_{true}$ is provided only to make it easier to evaluate the error in your predictions. You would not have $y_{true}$ in any real task.\n",
    "\n",
    "    (b) As introduced in the lecture notes, the feature mapping can substantially affect the regression results. We will consider two possible feature mappings:\n",
    "    $$\\begin{aligned} & \\phi_1\\left(x_1, x_2, x_3\\right)=\\left[1, x_1, x_2, x_3\\right]^{\\top} \\\\ & \\phi_2\\left(x_1, x_2, x_3\\right)=\\left[1, \\log x_1^2, \\log x_2^2, \\log x_3^2\\right]^{\\top}\\end{aligned}$$\n",
    "    Implement a python function `feature_mapping` to transform the input data matrix into a matrix\n",
    "    $$\\mathbf{X}=\\left[\\begin{array}{c}\\boldsymbol{\\phi}\\left(\\mathbf{x}_1\\right)^{\\top} \\\\ \\boldsymbol{\\phi}\\left(\\mathbf{x}_2\\right)^{\\top} \\\\ \\vdots \\\\ \\phi\\left(\\mathbf{x}_n\\right)^{\\top}\\end{array}\\right]$$\n",
    "    For example, `X = feature_mapping(X_in, 1)` would get you the first feature representation. Using your completed linear regression functions, compute the mean squared prediction error for each feature mapping (two numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "3. Implement a Perceptron classifier. Start by implementing the following functions\n",
    "\n",
    "    - a function `perceptron_train(X, y)` where `X` and `y` are $n \\times d$ and $n \\times 1$ matrices respectively. This function trains a Perceptron classifier on a training set of $n$ samples, each of which is a $d$-dimensional vector. The labels for the examples are in `y` and are $1$ or $−1$. The function should return `[theta, k]`, the final classification vector and the number of updates performed, respectively. You may assume that the input data provided to your function is linearly separable. No offset is used for simplicity.\n",
    "\n",
    "    - a function `perceptron_test(theta, X_test, y_test)` where `theta` is the classification vector to be used. `X_test` and `y_test` are $m\\times d$ and $m \\times 1$ matrices respectively, corresponding to m test examples and their true labels. The function should return `test_err`, the fraction of test examples which were misclassified.\n",
    "\n",
    "    For this problem, we have provided you two custom-created datasets. The dimension $d$ of both the datasets is $2$, for ease of plotting and visualization.\n",
    "\n",
    "    (a) Load data from `p3a.mat` (it contains `p3aX` and `p3ay`), and train your Perceptron classifier on it. Using the function `perceptron_test`, ensure that your classifier makes no errors on the training data. What is the angle between `theta` and the vector $(1,0)^\\top$? What is the number of updates $k_a$ required before the Perceptron algorithm converges?\n",
    "\n",
    "    (b) Repeat the above steps for data loaded from `p3b.mat` (it contains `p3bX` and `p3by`). What is the angle between `theta` and the vector $(1,0)^\\top$? What is the number of updates $k_b$?\n",
    "\n",
    "    (c) For parts (a) and (b), compute the geometric margins (the minimum distance of samples in one class to the decision hyperplane), $\\gamma^a_{geo}$ and $\\gamma^b_{geo}$, of your classifiers with respect to their corresponding training datasets. Recall that the distance of a point $\\mathbf{x}_t$ from the line $\\boldsymbol\\theta^\\top \\mathbf{x}=0$ is $|\\frac{\\boldsymbol\\theta^\\top \\mathbf{x}_t}{\\parallel\\boldsymbol\\theta\\parallel}|$.\n",
    "\n",
    "    (d) Plot the data (as points in the X-Y plane) from part (a), along with decision boundary that your Perceptron classifier computed. Create another plot, this time using data from part (b) and the corresponding decision boundary. Your plots should indicate the class of each point (e.g., by choosing different colors or symbols to mark the points from the two classes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
